# Multi-Environment Kubernetes - Interview Q&A

## Table of Contents
1. [Multi-Environment Architecture](#multi-environment-architecture)
2. [Resource Management](#resource-management)
3. [Scheduling & Placement](#scheduling--placement)
4. [High Availability](#high-availability)
5. [Real-World Scenarios](#real-world-scenarios)
6. [Week 3-4 Concepts](#week-3-4-concepts)

---

## Multi-Environment Architecture

### Q1: How would you design a multi-environment Kubernetes cluster (Dev, Staging, Production)?

**Answer:**

I would use **namespace-based isolation** with differentiated resource policies:

**Architecture Overview:**

```
Single Cluster with 3 Namespaces:
â”œâ”€ development (low resources, flexible)
â”œâ”€ staging (medium resources, prod-like)
â””â”€ production (high resources, strict)
```

**Key Components:**

**1. Namespace Separation**
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    environment: production
    tier: critical
```

**2. Resource Quotas per Environment**
```yaml
# Production: Generous
spec:
  hard:
    requests.cpu: "50"
    requests.memory: 100Gi
    pods: "200"

# Staging: Moderate
spec:
  hard:
    requests.cpu: "20"
    requests.memory: 40Gi
    pods: "100"

# Development: Minimal
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    pods: "50"
```

**3. LimitRanges for Consistency**
```yaml
# Production: Strict minimums
spec:
  limits:
  - type: Container
    min:
      cpu: "250m"
      memory: "256Mi"

# Development: Flexible
spec:
  limits:
  - type: Container
    min:
      cpu: "50m"
      memory: "64Mi"
```

**4. Node Affinity for Isolation**
```yaml
# Production pods MUST run on production nodes
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: environment
          operator: In
          values:
          - production
```

**5. Different Replica Counts**
- Development: 1 replica (cost-effective)
- Staging: 2 replicas (mirrors prod)
- Production: 5+ replicas (high availability)

**6. Different QoS Classes**
- Development: Burstable/BestEffort
- Staging: Burstable
- Production: Guaranteed for critical services

**Alternative Approach: Separate Clusters**

For larger organizations:
```
â”œâ”€ dev-cluster (cheap, relaxed policies)
â”œâ”€ staging-cluster (mirrors prod)
â””â”€ production-cluster (expensive, strict)
```

**Pros:**
- Complete isolation
- No resource competition
- Different Kubernetes versions possible

**Cons:**
- Higher cost
- More operational overhead
- Harder to promote between environments

**My Recommendation:**
- Small/Medium: Namespace-based (same cluster)
- Large/Enterprise: Separate clusters
- Hybrid: Dev in one cluster, Staging+Prod in another

---

### Q2: What are the advantages and disadvantages of namespace-based vs cluster-based environment separation?

**Answer:**

**Namespace-Based (Single Cluster):**

**Advantages:**
```
âœ… Lower cost (shared infrastructure)
âœ… Easier cluster management (one cluster to maintain)
âœ… Easy resource sharing
âœ… Simple promotion (kubectl apply -n staging â†’ -n production)
âœ… Centralized monitoring
âœ… Lower operational overhead
```

**Disadvantages:**
```
âŒ Shared control plane (noisy neighbor risk)
âŒ Blast radius (cluster failure affects all environments)
âŒ Same Kubernetes version (can't test upgrades)
âŒ Resource competition possible
âŒ Harder to implement strong isolation
âŒ Network policies required for isolation
```

**Cluster-Based (Separate Clusters):**

**Advantages:**
```
âœ… Complete isolation
âœ… No resource competition
âœ… Independent failures (dev crash doesn't affect prod)
âœ… Different K8s versions (safe testing)
âœ… Better security boundaries
âœ… Easier compliance (separate audit logs)
```

**Disadvantages:**
```
âŒ Higher cost (3x infrastructure)
âŒ More operational overhead (3 clusters to manage)
âŒ Complex promotion (different contexts)
âŒ Harder monitoring (multiple dashboards)
âŒ Resource waste (can't share capacity)
```

**Comparison Table:**

| Factor | Namespace-Based | Cluster-Based |
|--------|----------------|---------------|
| **Cost** | Low | High |
| **Isolation** | Moderate | Complete |
| **Operations** | Simple | Complex |
| **Blast Radius** | High | Low |
| **Resource Sharing** | Easy | Impossible |
| **Best For** | Small/Medium teams | Large/Enterprise |

**My Approach:**

```
Startup/Small Team:
â””â”€ Single cluster, namespace-based

Growing Team:
â”œâ”€ Dev cluster (cheap, experimental)
â””â”€ Prod cluster (expensive, stable)

Enterprise:
â”œâ”€ Dev cluster
â”œâ”€ Staging cluster
â””â”€ Production cluster(s)
```

---

## Resource Management

### Q3: How do you prevent development workloads from affecting production in a shared cluster?

**Answer:**

**Multi-Layered Approach:**

**1. ResourceQuotas (Hard Limits)**
```yaml
# Production: High quota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: prod-quota
  namespace: production
spec:
  hard:
    requests.cpu: "50"      # Production gets most resources
    requests.memory: 100Gi
    pods: "200"
---
# Development: Low quota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: development
spec:
  hard:
    requests.cpu: "10"      # Development limited
    requests.memory: 20Gi
    pods: "50"
```

**Result:** Dev can't consume more than 10 CPUs even if they try

**2. Node Affinity (Physical Separation)**
```yaml
# Production pods ONLY on production nodes
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: environment
            operator: In
            values:
            - production
```

**Result:** Dev pods can't schedule on production nodes

**3. Taints + Tolerations (Additional Protection)**
```bash
# Taint production nodes
kubectl taint nodes prod-node-1 environment=production:NoSchedule

# Only production pods tolerate this
tolerations:
- key: environment
  value: production
  effect: NoSchedule
```

**Result:** Only pods with toleration can run on production nodes

**4. Priority Classes (Eviction Order)**
```yaml
# Production pods get high priority
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: production-priority
value: 1000000
---
# Development pods get low priority
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: dev-priority
value: 100000
```

**Result:** Under pressure, dev pods evicted before production

**5. Network Policies (Network Isolation)**
```yaml
# Production can ONLY talk to production
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: prod-isolation
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          environment: production
```

**Result:** Dev can't access production services

**6. LimitRanges (Prevent Resource Hogging)**
```yaml
# Cap individual pod sizes in dev
apiVersion: v1
kind: LimitRange
metadata:
  name: dev-limits
  namespace: development
spec:
  limits:
  - type: Container
    max:
      cpu: "2"          # No single pod can hog resources
      memory: "2Gi"
```

**Complete Protection Stack:**

```
Layer 1: ResourceQuota (namespace-level limits)
Layer 2: Node Affinity (physical separation)
Layer 3: Taints (prevent scheduling)
Layer 4: Priority Classes (eviction order)
Layer 5: Network Policies (network isolation)
Layer 6: LimitRanges (pod-level caps)
```

**Real Example:**

Before protection:
```
Dev team deployed 100 replicas by accident
â†’ Consumed entire cluster CPU
â†’ Production pods crashed
â†’ Incident!
```

After protection:
```
Dev team deployed 100 replicas
â†’ Only 50 pods created (quota limit)
â†’ All on dev nodes (affinity)
â†’ Production unaffected
â†’ No incident!
```

---

## Scheduling & Placement

### Q4: Explain the difference between nodeSelector, node affinity, and pod anti-affinity.

**Answer:**

**1. nodeSelector (Simple Label Matching)**

**Purpose:** Schedule pods on nodes with specific labels

**Syntax:**
```yaml
spec:
  nodeSelector:
    environment: production
    disktype: ssd
```

**Characteristics:**
- âœ… Simple and easy
- âœ… AND logic only (all labels must match)
- âŒ No OR logic
- âŒ No "not in" logic
- âŒ No preferences (all or nothing)

**Use Case:** 80% of scheduling needs

**2. Node Affinity (Advanced Node Selection)**

**Purpose:** Advanced rules for node selection with OR logic and preferences

**Syntax:**
```yaml
spec:
  affinity:
    nodeAffinity:
      # HARD requirement
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: environment
            operator: In        # OR logic possible
            values:
            - production
            - staging
      
      # SOFT preference
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80
        preference:
          matchExpressions:
          - key: disktype
            operator: In
            values:
            - ssd
```

**Characteristics:**
- âœ… OR logic (multiple values)
- âœ… NOT logic (NotIn, DoesNotExist)
- âœ… Preferences with weights
- âœ… Required + Preferred combined
- âŒ More complex

**Operators:**
- In, NotIn, Exists, DoesNotExist, Gt, Lt

**Use Case:** Complex placement rules

**3. Pod Anti-Affinity (Pod Spread)**

**Purpose:** Control pod placement relative to OTHER pods

**Syntax:**
```yaml
spec:
  affinity:
    podAntiAffinity:
      # HARD: MUST be on different nodes
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app: backend
        topologyKey: kubernetes.io/hostname  # Different nodes
      
      # SOFT: PREFER different zones
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: backend
          topologyKey: topology.kubernetes.io/zone  # Different zones
```

**Characteristics:**
- âœ… Spread pods apart
- âœ… High availability
- âœ… Zone/node/region spreading
- âŒ Can prevent scheduling if too strict

**Topology Keys:**
- `kubernetes.io/hostname` â†’ Different nodes
- `topology.kubernetes.io/zone` â†’ Different zones
- `topology.kubernetes.io/region` â†’ Different regions

**Use Case:** High availability, disaster recovery

**Comparison Table:**

| Feature | nodeSelector | Node Affinity | Pod Anti-Affinity |
|---------|-------------|---------------|-------------------|
| **Logic** | AND only | OR, NOT, AND | Pod-to-pod |
| **Complexity** | Simple | Medium | High |
| **Preferences** | No | Yes | Yes |
| **Use Case** | Basic placement | Advanced placement | Pod spreading |
| **Required** | Yes | Yes/No | Yes/No |

**When to Use:**

```
nodeSelector:
  âœ… Simple requirements (environment=production)
  âœ… 80% of use cases
  
Node Affinity:
  âœ… OR logic needed (prod OR staging)
  âœ… Preferences (prefer SSD but HDD ok)
  âœ… NOT logic (NOT in us-east)
  
Pod Anti-Affinity:
  âœ… High availability (spread across nodes/zones)
  âœ… Performance (avoid resource contention)
  âœ… Disaster recovery
```

**Real Example - Production Backend:**

```yaml
spec:
  replicas: 5
  template:
    spec:
      # nodeSelector: Simple requirement
      nodeSelector:
        environment: production
      
      # Node Affinity: Advanced requirement + preference
      affinity:
        nodeAffinity:
          # MUST be production or staging
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: environment
                operator: In
                values:
                - production
                - staging
          
          # PREFER SSD nodes
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 80
            preference:
              matchExpressions:
              - key: disktype
                operator: In
                values:
                - ssd
        
        # Pod Anti-Affinity: Spread across zones
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: backend
            topologyKey: topology.kubernetes.io/zone
```

**Result:**
- âœ… Only on production/staging nodes (nodeSelector + affinity)
- âœ… Prefers SSD but works on HDD (preferred affinity)
- âœ… Spread across zones for HA (pod anti-affinity)

---

## High Availability

### Q5: How would you ensure high availability for a critical production service?

**Answer:**

**Multi-Layered HA Strategy:**

**1. Multiple Replicas**
```yaml
spec:
  replicas: 5  # Minimum 3, recommend 5+ for critical services
```

**2. Pod Anti-Affinity (Zone Distribution)**
```yaml
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchLabels:
        app: payment-service
    topologyKey: topology.kubernetes.io/zone  # Different zones
```

**Result:** Service survives zone failure

**3. Guaranteed QoS (Resource Stability)**
```yaml
resources:
  requests:
    cpu: "1000m"
    memory: "1Gi"
  limits:
    cpu: "1000m"    # Same as request = Guaranteed
    memory: "1Gi"
```

**Result:** Pod never evicted due to resource pressure

**4. PodDisruptionBudget (Protect During Updates)**
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: payment-pdb
spec:
  minAvailable: 3  # Always keep 3 running
  selector:
    matchLabels:
      app: payment-service
```

**Result:** Rolling updates can't take down too many pods

**5. Health Checks (Auto-Recovery)**
```yaml
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 10
  periodSeconds: 5
```

**Result:** Failed pods automatically restarted, unhealthy pods removed from service

**6. Rolling Update Strategy**
```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1        # Add 1 extra pod
    maxUnavailable: 1  # Remove 1 at a time
```

**Result:** Zero-downtime deployments

**7. HPA (Handle Traffic Spikes)**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  minReplicas: 5
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        averageUtilization: 70
```

**Result:** Automatically scales under load

**8. Multi-Zone LoadBalancer**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: payment-service
spec:
  type: LoadBalancer
  selector:
    app: payment-service
```

**Result:** Traffic distributed across zones

**9. StatefulSet for Databases**
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
spec:
  replicas: 3  # Odd number for quorum
  serviceName: database
  podManagementPolicy: OrderedReady
```

**Result:** Ordered, stable database cluster

**10. Backup & Disaster Recovery**
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup
spec:
  schedule: "0 */6 * * *"  # Every 6 hours
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: backup-tool
```

**Complete HA Checklist:**

```
âœ… 5+ replicas
âœ… Zone anti-affinity
âœ… Guaranteed QoS
âœ… PodDisruptionBudget
âœ… Liveness & readiness probes
âœ… Rolling updates
âœ… HPA configured
âœ… Multi-zone service
âœ… StatefulSet for stateful apps
âœ… Regular backups
âœ… Monitoring & alerts
âœ… Runbooks & on-call
```

**Availability Math:**

```
Single pod:
- Uptime: 99.9%
- Downtime: 43 minutes/month

5 pods across 3 zones:
- Uptime: 99.99%+
- Downtime: 4 minutes/month

With all HA measures:
- Uptime: 99.999%
- Downtime: 26 seconds/month
```

**Real Example - Payment Service:**

Before HA:
```
- 1 replica
- BestEffort QoS
- No health checks
- No anti-affinity

Result: Downtime 2 hours/month
```

After HA:
```
- 5 replicas
- Guaranteed QoS  
- Health checks configured
- Zone anti-affinity
- PodDisruptionBudget
- HPA (5-20 replicas)

Result: Downtime 30 seconds/month (99.99%)
```

---

## Week 3-4 Concepts

### Q6: Summarize the key learnings from Kubernetes Weeks 3-4.

**Answer:**

**Week 3: Advanced Scheduling**

**Day 15-16: DaemonSets & StatefulSets**
- **DaemonSets:** One pod per node (logging, monitoring)
- **StatefulSets:** Ordered, stable pods (databases)
- **Key Learning:** Different workload types need different controllers

**Day 17-18: Jobs & CronJobs**
- **Jobs:** Run to completion (batch processing)
- **CronJobs:** Scheduled tasks (backups, reports)
- **Key Learning:** Not everything needs to run forever

**Day 19-20: Manual Scheduling & Node Selection**
- **nodeName:** Direct assignment (don't use in prod)
- **nodeSelector:** Label-based (simple, effective)
- **Node Affinity:** Advanced rules (OR logic, preferences)
- **Key Learning:** Control WHERE pods run

**Day 21-22: Taints & Tolerations**
- **Taints:** Repel pods from nodes
- **Tolerations:** Allow pods to schedule
- **Key Learning:** Dedicated node pools for special workloads

**Week 4: Resource Management**

**Day 23-24: Resource Limits & Requests**
- **Requests:** Guaranteed minimum
- **Limits:** Maximum allowed
- **QoS Classes:** Guaranteed > Burstable > BestEffort
- **ResourceQuotas:** Namespace-level limits
- **LimitRanges:** Default policies
- **Key Learning:** Resource management prevents chaos

**Day 25-26: HPA**
- **Auto-scaling:** Based on CPU/memory metrics
- **Min/Max Replicas:** Safety bounds
- **Key Learning:** Handle traffic automatically

**Key Takeaways:**

```
Scheduling:
- Control WHERE (node affinity, taints)
- Control WHEN (jobs, cronjobs)
- Control HOW (daemonsets, statefulsets)

Resources:
- Always set requests & limits
- Use appropriate QoS class
- Implement quotas per namespace
- Monitor and optimize

High Availability:
- Multiple replicas
- Pod anti-affinity
- Health checks
- PodDisruptionBudget
- HPA for scaling
```

This is the foundation for production-ready Kubernetes! ðŸš€

---

This interview Q&A covers all essential multi-environment concepts!
