# Day 27-28: Week 3-4 Review & Multi-Environment Project

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Week 3-4 Topics Covered](#week-3-4-topics-covered)
- [Multi-Environment Architecture](#multi-environment-architecture)
- [Prerequisites](#prerequisites)
- [Project Structure](#project-structure)
- [Quick Start](#quick-start)
- [Learning Objectives](#learning-objectives)
- [Best Practices](#best-practices)
- [Additional Resources](#additional-resources)

## ğŸ“– Overview

This capstone project brings together everything learned in Weeks 3-4, implementing a complete multi-environment deployment with proper workload scheduling, resource management, and production-ready patterns.

**What You'll Build:**
- 3 environments (Development, Staging, Production)
- Different scheduling policies per environment
- Resource quotas and limits
- Node affinity and anti-affinity
- Complete CI/CD-ready structure

## ğŸ¯ Week 3-4 Topics Covered

### Week 3: Advanced Pod Scheduling

**Day 15-16: DaemonSets & StatefulSets**
- DaemonSets for node-level services
- StatefulSets for stateful applications
- Persistent storage management
- Ordered deployment/scaling

**Day 17-18: Jobs & CronJobs**
- Batch processing with Jobs
- Scheduled tasks with CronJobs
- Job completion and cleanup
- Parallel execution patterns

**Day 19-20: Manual Scheduling & Node Selection**
- nodeName direct assignment
- nodeSelector label-based scheduling
- Node affinity (required/preferred)
- Topology spread constraints

**Day 21-22: Taints, Tolerations & Node Affinity**
- Taints to repel pods from nodes
- Tolerations to allow scheduling
- Advanced node affinity patterns
- Multi-zone deployments

### Week 4: Resource Management & Optimization

**Day 23-24: Resource Limits & Requests**
- CPU and memory management
- QoS classes (Guaranteed, Burstable, BestEffort)
- ResourceQuotas for namespaces
- LimitRanges for defaults

**Day 25-26: Horizontal Pod Autoscaler (HPA)**
- Automatic scaling based on metrics
- CPU/Memory-based scaling
- Custom metrics scaling
- Scaling best practices

**Day 27-28: Week Review & Multi-Environment Project** â† You are here!

## ğŸ—ï¸ Multi-Environment Architecture

### Environment Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kubernetes Cluster                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Production Namespace                              â”‚     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚  â”‚ â€¢ High resource quotas (50 CPU, 100Gi RAM)       â”‚     â”‚
â”‚  â”‚ â€¢ Guaranteed QoS for critical services           â”‚     â”‚
â”‚  â”‚ â€¢ Node affinity: production nodes only           â”‚     â”‚
â”‚  â”‚ â€¢ Anti-affinity: spread across zones             â”‚     â”‚
â”‚  â”‚ â€¢ Strict limits enforcement                      â”‚     â”‚
â”‚  â”‚ â€¢ 3 replicas minimum                             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Staging Namespace                                 â”‚     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚  â”‚ â€¢ Medium resource quotas (20 CPU, 40Gi RAM)      â”‚     â”‚
â”‚  â”‚ â€¢ Burstable QoS                                  â”‚     â”‚
â”‚  â”‚ â€¢ Node affinity: staging nodes preferred         â”‚     â”‚
â”‚  â”‚ â€¢ Anti-affinity: spread across nodes             â”‚     â”‚
â”‚  â”‚ â€¢ Moderate limits                                â”‚     â”‚
â”‚  â”‚ â€¢ 2 replicas                                     â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Development Namespace                             â”‚     â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”‚
â”‚  â”‚ â€¢ Lower resource quotas (10 CPU, 20Gi RAM)       â”‚     â”‚
â”‚  â”‚ â€¢ Burstable/BestEffort QoS                       â”‚     â”‚
â”‚  â”‚ â€¢ Node affinity: development nodes               â”‚     â”‚
â”‚  â”‚ â€¢ No anti-affinity (cost saving)                 â”‚     â”‚
â”‚  â”‚ â€¢ Flexible limits                                â”‚     â”‚
â”‚  â”‚ â€¢ 1 replica                                      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Node Labeling Strategy

```bash
# Production nodes
kubectl label nodes prod-node-{1,2,3} \
  environment=production \
  tier=high-performance \
  zone=us-west-1a

# Staging nodes
kubectl label nodes staging-node-{1,2} \
  environment=staging \
  tier=medium-performance \
  zone=us-west-1b

# Development nodes
kubectl label nodes dev-node-1 \
  environment=development \
  tier=standard \
  zone=us-west-1c
```

## ğŸ“‹ Prerequisites

- Kubernetes cluster (v1.24+)
- kubectl configured with admin access
- At least 3 worker nodes (ideally 6+ for full separation)
- Metrics Server installed
- Basic understanding of all Week 3-4 topics

### Verify Setup

```bash
# Check cluster
kubectl cluster-info

# Check nodes
kubectl get nodes

# Check metrics server
kubectl top nodes

# Verify permissions
kubectl auth can-i create namespace
kubectl auth can-i create resourcequota
```

## ğŸ“ Project Structure

```
day27-28-week-review/
â”œâ”€â”€ README.md
â”œâ”€â”€ GUIDEME.md
â”œâ”€â”€ COMMAND-CHEATSHEET.md
â”œâ”€â”€ INTERVIEW-QA.md
â”œâ”€â”€ TROUBLESHOOTING.md
â”œâ”€â”€ LINKEDIN-POSTS.md
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ 01-infrastructure/
â”‚   â”‚   â”œâ”€â”€ namespaces.yaml
â”‚   â”‚   â”œâ”€â”€ node-labels.sh
â”‚   â”‚   â””â”€â”€ resource-quotas.yaml
â”‚   â”œâ”€â”€ 02-development/
â”‚   â”‚   â”œâ”€â”€ backend-dev.yaml
â”‚   â”‚   â”œâ”€â”€ frontend-dev.yaml
â”‚   â”‚   â”œâ”€â”€ database-dev.yaml
â”‚   â”‚   â””â”€â”€ jobs-dev.yaml
â”‚   â”œâ”€â”€ 03-staging/
â”‚   â”‚   â”œâ”€â”€ backend-staging.yaml
â”‚   â”‚   â”œâ”€â”€ frontend-staging.yaml
â”‚   â”‚   â”œâ”€â”€ database-staging.yaml
â”‚   â”‚   â””â”€â”€ cronjobs-staging.yaml
â”‚   â”œâ”€â”€ 04-production/
â”‚   â”‚   â”œâ”€â”€ backend-prod.yaml
â”‚   â”‚   â”œâ”€â”€ frontend-prod.yaml
â”‚   â”‚   â”œâ”€â”€ database-prod.yaml
â”‚   â”‚   â”œâ”€â”€ cache-prod.yaml
â”‚   â”‚   â””â”€â”€ monitoring-prod.yaml
â”‚   â”œâ”€â”€ 05-shared-services/
â”‚   â”‚   â”œâ”€â”€ logging-daemonset.yaml
â”‚   â”‚   â”œâ”€â”€ monitoring-daemonset.yaml
â”‚   â”‚   â””â”€â”€ ingress-controller.yaml
â”‚   â””â”€â”€ 06-complete-stack/
â”‚       â””â”€â”€ full-deployment.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup-cluster.sh
â”‚   â”œâ”€â”€ deploy-dev.sh
â”‚   â”œâ”€â”€ deploy-staging.sh
â”‚   â”œâ”€â”€ deploy-prod.sh
â”‚   â”œâ”€â”€ validate-deployment.sh
â”‚   â””â”€â”€ cleanup.sh
â””â”€â”€ docs/
    â”œâ”€â”€ architecture.md
    â””â”€â”€ decision-log.md
```

## ğŸš€ Quick Start

### Step 1: Setup Infrastructure

```bash
# Label nodes
./scripts/setup-cluster.sh

# Create namespaces and quotas
kubectl apply -f examples/01-infrastructure/
```

### Step 2: Deploy Development Environment

```bash
# Deploy dev environment
./scripts/deploy-dev.sh

# Verify
kubectl get all -n development
```

### Step 3: Deploy Staging Environment

```bash
# Deploy staging
./scripts/deploy-staging.sh

# Verify
kubectl get all -n staging
```

### Step 4: Deploy Production Environment

```bash
# Deploy production
./scripts/deploy-prod.sh

# Verify
kubectl get all -n production
```

### Step 5: Validate Complete Setup

```bash
# Run validation
./scripts/validate-deployment.sh

# Check resource usage
kubectl top pods -A
```

## ğŸ“ Learning Objectives

By completing this project, you will:

1. âœ… Apply all Week 3-4 concepts in real scenario
2. âœ… Build production-ready multi-environment setup
3. âœ… Implement proper resource governance
4. âœ… Use advanced scheduling patterns
5. âœ… Configure environment-specific policies
6. âœ… Create reusable deployment templates
7. âœ… Understand CI/CD integration patterns
8. âœ… Master troubleshooting multi-env issues

## ğŸŒ Environment Specifications

### Development Environment

**Purpose**: Fast iteration, experimentation

**Characteristics:**
```yaml
Replicas: 1
QoS: Burstable or BestEffort
Resources: Minimal (100m CPU, 128Mi RAM)
Node Affinity: development nodes
Anti-Affinity: None (cost saving)
ResourceQuota: 10 CPU, 20Gi RAM, 50 pods
LimitRange: Flexible (50m-2 CPU, 64Mi-2Gi)
```

**Use Cases:**
- Feature development
- Bug fixes
- Integration testing
- Experimentation

### Staging Environment

**Purpose**: Pre-production validation

**Characteristics:**
```yaml
Replicas: 2
QoS: Burstable
Resources: Medium (250m CPU, 256Mi RAM)
Node Affinity: staging nodes preferred
Anti-Affinity: Spread across nodes
ResourceQuota: 20 CPU, 40Gi RAM, 100 pods
LimitRange: Moderate (100m-4 CPU, 128Mi-4Gi)
```

**Use Cases:**
- Integration testing
- Performance testing
- UAT
- Release validation

### Production Environment

**Purpose**: Live customer traffic

**Characteristics:**
```yaml
Replicas: 3+ (minimum)
QoS: Guaranteed for critical services
Resources: Generous (500m CPU, 512Mi RAM minimum)
Node Affinity: production nodes required
Anti-Affinity: Spread across zones
ResourceQuota: 50 CPU, 100Gi RAM, 200 pods
LimitRange: Strict (250m-8 CPU, 256Mi-16Gi)
```

**Use Cases:**
- Customer-facing applications
- Critical services
- High availability required
- Performance critical

## âœ… Best Practices Applied

### 1. Environment Isolation

```yaml
# Production namespace with strict isolation
apiVersion: v1
kind: Namespace
metadata:
  name: production
  labels:
    environment: production
    tier: critical
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: prod-quota
  namespace: production
spec:
  hard:
    requests.cpu: "50"
    requests.memory: 100Gi
    limits.cpu: "100"
    limits.memory: 200Gi
    pods: "200"
---
apiVersion: v1
kind: LimitRange
metadata:
  name: prod-limits
  namespace: production
spec:
  limits:
  - type: Container
    min:
      cpu: "250m"
      memory: "256Mi"
    max:
      cpu: "8"
      memory: "16Gi"
    defaultRequest:
      cpu: "500m"
      memory: "512Mi"
    default:
      cpu: "1000m"
      memory: "1Gi"
```

### 2. Progressive Deployment

```
Development â†’ Staging â†’ Production

1. Deploy to dev
2. Run tests
3. Promote to staging
4. Run integration tests
5. Manual approval
6. Deploy to production
7. Monitor & validate
```

### 3. High Availability

```yaml
# Production deployment with HA
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-prod
  namespace: production
spec:
  replicas: 5  # Minimum 3 for HA
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: backend
      environment: production
  template:
    metadata:
      labels:
        app: backend
        environment: production
    spec:
      # Node affinity: production nodes only
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: environment
                operator: In
                values:
                - production
        # Pod anti-affinity: spread across zones
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app: backend
                environment: production
            topologyKey: topology.kubernetes.io/zone
      # Guaranteed QoS for stability
      containers:
      - name: backend
        image: backend:v1.0.0
        resources:
          requests:
            cpu: "1000m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "1Gi"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
```

### 4. Resource Optimization

```yaml
# Development: Minimal resources
resources:
  requests:
    cpu: "100m"
    memory: "128Mi"
  limits:
    cpu: "250m"
    memory: "256Mi"

# Staging: Medium resources
resources:
  requests:
    cpu: "250m"
    memory: "256Mi"
  limits:
    cpu: "500m"
    memory: "512Mi"

# Production: Generous resources
resources:
  requests:
    cpu: "1000m"
    memory: "1Gi"
  limits:
    cpu: "1000m"  # Guaranteed QoS
    memory: "1Gi"
```

### 5. Shared Services

```yaml
# DaemonSet for logging (all nodes)
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1
        resources:
          requests:
            cpu: "100m"
            memory: "200Mi"
          limits:
            cpu: "200m"
            memory: "400Mi"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
```

## ğŸ” Comparison Matrix

| Feature | Development | Staging | Production |
|---------|-------------|---------|------------|
| **Replicas** | 1 | 2 | 3-5+ |
| **QoS Class** | Burstable/BestEffort | Burstable | Guaranteed |
| **CPU Request** | 100m | 250m | 1000m |
| **Memory Request** | 128Mi | 256Mi | 1Gi |
| **Anti-Affinity** | None | Node-level | Zone-level |
| **Resource Quota** | 10 CPU, 20Gi | 20 CPU, 40Gi | 50 CPU, 100Gi |
| **Max Pods** | 50 | 100 | 200 |
| **Health Checks** | Optional | Required | Required |
| **Monitoring** | Basic | Standard | Advanced |
| **Backup** | None | Daily | Hourly |
| **Cost** | Low | Medium | High |

## ğŸ“Š Resource Allocation

### Cluster Resource Distribution

```
Total Cluster: 60 CPUs, 120Gi RAM

Production:   50 CPUs (83%), 100Gi RAM (83%)
Staging:      20 CPUs (33%), 40Gi RAM (33%)
Development:  10 CPUs (17%), 20Gi RAM (17%)

Note: Quotas are limits, not reservations
Actual usage typically much lower
```


### Official Documentation
- [Resource Management](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)
- [Assigning Pods to Nodes](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)
- [Multi-tenancy](https://kubernetes.io/docs/concepts/security/multi-tenancy/)

### Learning Resources
- Command Cheatsheet: [COMMAND-CHEATSHEET.md](./COMMAND-CHEATSHEET.md)
- Interview Questions: [INTERVIEW-QA.md](./INTERVIEW-QA.md)
- Step-by-Step Guide: [GUIDEME.md](./GUIDEME.md)
- Troubleshooting: [TROUBLESHOOTING.md](./TROUBLESHOOTING.md)

---

## ğŸ¤ Contributing

This is a capstone project combining all Week 3-4 learnings!

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ‘¨â€ğŸ’» Author

**Your Name**
- GitHub: (https://github.com/sai-look-hub/CKA-75/new/main)
- LinkedIn: www.linkedin.com/in/saikumara

---

**Happy Learning! ğŸš€**

*Master Kubernetes workload scheduling and build production-ready multi-environment deployments!*
